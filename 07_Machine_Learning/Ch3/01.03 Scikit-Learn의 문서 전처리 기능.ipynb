{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "#matplotlib.rc('font', family=\"D2Coding\")\n",
    "# [f.fname for f in matplotlib.font_manager.fontManager.ttflist]\n",
    "font_location = 'C:\\\\Users\\\\62190\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NanumBarunGothic.ttf' # ex - 또는 ./utils/NanumBarunGothic.ttf\n",
    "font_name = fm.FontProperties(fname = font_location).get_name()\n",
    "matplotlib.rc('font', family = font_name)\n",
    "\n",
    "from patsy import *\n",
    "# 경고 무시\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# 자주 사용하는 패키지를 임포트\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import sklearn as sk\n",
    "import pgmpy\n",
    "import scipy.stats\n",
    "\n",
    "# matplotlib 설정\n",
    "mpl.use('Agg')\n",
    "\n",
    "# seaborn 설정\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_color_codes()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 5,\n",
       " 'the': 9,\n",
       " 'first': 4,\n",
       " 'document': 3,\n",
       " '.': 0,\n",
       " 'second': 8,\n",
       " 'and': 2,\n",
       " 'third': 10,\n",
       " 'one': 7,\n",
       " '?': 1,\n",
       " 'last': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 12,\n",
       " 'is the': 2,\n",
       " 'the first': 7,\n",
       " 'first document': 1,\n",
       " 'the second': 9,\n",
       " 'second second': 6,\n",
       " 'second document': 5,\n",
       " 'and the': 0,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'is this': 3,\n",
       " 'this the': 13,\n",
       " 'the last': 8,\n",
       " 'last document': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 3, 'is': 2, 'first': 1, 'document': 0},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "- 이 단어가 이 문서에 쓰인 비율이 얼마일까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x300000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1786336 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time hv.transform(twenty.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import json\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "\n",
    "f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "json = json.loads(f.read())\n",
    "\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [\n",
    "    w for w in hannanum.nouns(\" \".join(cell)) \n",
    "    if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD7CAYAAAB37B+tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATUElEQVR4nO3dfYxcZ3mG8csx3sUYUJoCtVMnaSPjB5dVHJg6qJA0aTEgS1gGFRPhgLBogKjQplICguI0IRJIpNStQBWgQhSEVTDY5aMJUVMMpaTQlI4IsDF+YrWbiNSLQKpUUtLM+qt/nFl3Yu/HmfHOzrzr6ydZO3PmzJx7VuN73n3PzDnLTp48iSSpXOcNOoAk6exY5JJUOItckgpnkUtS4SxySSrc0xZ7g81mcxTYBEwCxxd7+5JUqOXAGuC7jUaj1XnDohc5VYl/awDblaSl4Crg/s4FgyjySYD169czMjLS0wOMj48zNja2oKEWU8n5S84O5h+kkrPD4PNPTU3x8MMPQ7tDOw2iyI8DjIyMMDo62vODnM19h0HJ+UvODuYfpJKzw9DkP2NK2p2dklQ4i1ySCmeRS1LhLHJJKpxFLkmFs8glqXAWuSQVrrginzp6nEajwdRRv90vSVBgkY+sWM7Wm77MyIrlg44iSUOhuCKXJD2VRS5Jhat1rJWIeCPw3vbVezPz5ojYDOwGVgJ7M3NXnzJKkuYw74g8Ip4BfAS4GtgIXBURW4E7gW3ABmBTRGzpZ1BJ0szqTK0sb6+3CljR/vdz4HBmTmTmMWAPsL1vKSVJs5q3yDPzceAW4BDwGPAIcCFPPSbuJLC2D/kkSfOYd448Ii4D3gJcAvw31eh7PXCyY7VlwIluNjw+Pt7N6qc0Go1Tl5vNZk+PMQzMPjjmH5ySs8Pw5q+zs/NVwIHM/ClARNwF3MxTD26+GjjSzYbHxsbO+iDtnaVekmazafYBMf/glJwdBp+/1WrNOgCuU+TfB+6IiFXAE8BW4AHguohYB0wAO6h2fkqSFlmdOfL7gM8CTeAHVDs7bwN2AvuBg1Tz5/v6FVKSNLtanyPPzA8BHzpt8QGqjyNKkgbIb3ZKUuEsckkqnEUuSYWzyCWpcBa5JBXOIpekwlnkklQ4i1ySCmeRS1LhLHJJKpxFLkmFs8glqXAWuSQVziKXpMJZ5JJUOItckgpX5+TL1wPv7Fj068BngC8Bu4GVwN7M3NWXhJKkOdU51dsnM/PyzLwcuA74KdXZgu4EtgEbgE0RsaWvSSVJM+p2auVjwJ8AlwKHM3MiM48Be4DtCx1OkjS/2kUeEZuBlZn5BeBCYLLj5klg7QJnkyTVUOvky21vp5oTh+oN4GTHbcuAE91seHx8vJvVT2k0GqcuN5vNnh5jGJh9cMw/OCVnh+HNX6vII2IEuBrY2V70GLCmY5XVwJFuNjw2Nsbo6Gg3dzlDZ6mXpNlsmn1AzD84JWeHwedvtVqzDoDrjsgvAx7OzF+0rz8ARESsAyaAHVQ7PyVJi6zuHPmlVKNwADLzSarR+X7gIHAI2LfQ4SRJ86s1Is/MzwOfP23ZAWBjP0JJkurzm52SVDiLXJIKZ5FLUuEsckkqnEUuSYWzyCWpcBa5JBXOIpekwlnkklQ4i1ySCmeRS1LhLHJJKpxFLkmFs8glqXAWuSQVziKXpMLVPWfnVuBWYBVwX2beGBGbqU7GvBLYm5m7+hdTkjSbeUfkEXEp8HHgNVTn7nxxRGyhOkfnNmADsKm9TJK0yOpMrbyWasT9WGYeBa4FngAOZ+ZEZh4D9gDb+5hTkjSLOlMr64CpiPgKcDFwN/AQMNmxziSwduHjSZLmU6fInwb8NnAN8D/AV4D/BU52rLMMONHNhsfHx7tZ/ZRGo3HqcrPZ7OkxhoHZB8f8g1Nydhje/HWK/CfA1zLzZwAR8UWqaZTjHeusBo50s+GxsTFGR0e7ucsZOku9JM1m0+wDYv7BKTk7DD5/q9WadQBcp8jvBj4dEecDjwNbgH3AeyJiHTAB7KDa+SlJWmTz7uzMzAeAO4D7gYPAo8DHgJ3A/vayQ1TlLklaZLU+R56Zd3LmiPsAsHHBE0mSuuI3OyWpcBa5JBXOIpekwlnkklQ4i1ySCmeRS1LhLHJJKpxFLkmFs8glqXAWuSQVziKXpMJZ5JJUOItckgpnkUtS4SxySSqcRS5Jhat1YomI+AbwPOBoe9HbgWcBu4GVwN7M3NWXhJKkOc1b5BGxDFgPXJKZx9rLVgIJXA38GLgnIrZk5r39DCtJOlOdEXm0f94XEb8M/DXwQ+BwZk4ARMQeYDtgkUvSIqszR/5LVOfnfC3wcuAG4GJgsmOdSWDtgqeTJM1r3hF5Zn4H+M709Yj4FHA7cH/HasuAE91seHx8vJvVT2k0GqcuN5vNnh5jGJh9cMw/OCVnh+HNX2eO/EpgNDMPtBctAx4B1nSstho40s2Gx8bGGB0d7eYuZ+gs9ZI0m02zD4j5B6fk7DD4/K1Wa9YBcJ058vOB2yPipcAK4M1U0yufj4h1wASwA7hzYeJKkrox7xx5Zt4N3AN8D2gCd7anW3YC+4GDwCFgX/9iSpJmU+tz5Jl5C3DLacsOABv7EUqSVJ/f7JSkwlnkklQ4i1ySCmeRS1LhLHJJKpxFLkmFs8glqXAWuSQVziKXpMJZ5JJUOItckgpnkUtS4SxySSqcRS5JhbPIJalwFrkkFa7WiSUAIuLDwHMyc2dEbAZ2AyuBvZm5q18BJUlzqzUij4iXU52rk4hYSXV+zm3ABmBTRGzpW8I5TB09ztTR44PYtCQNjXmLPCIuAD4AfLC96ArgcGZOZOYxYA+wvX8RZzeyYjkjK5YPYtOSNDTqTK18AngfcFH7+oXAZMftk8Dabjc8Pj7e7V0AaDQaZyxrNps9PdYglZh5WsnZwfyDVHJ2GN78cxZ5RFwP/DgzD0TEzvbi84CTHastA050u+GxsTFGR0e7vduMZir3YdZsNovLPK3k7GD+QSo5Oww+f6vVmnUAPN+I/FpgTUQ8CFwAPBO4BOicmF4NHFmAnJKkHsxZ5Jn5iunL7RH5NcANwOGIWAdMADuodn5Kkgag68+RZ+aTwE5gP3AQOATsW9hYkqS6an+OPDPvAu5qXz4AbOxPJElSN/xmpyQVziKXpMJZ5JJUOItckgpnkUtS4SxySSqcRS5JhbPIJalwFrkkFW5JFPn0ySVa7Z+ebELSuWRJFPnIiuVsvenLjLZ/erIJSeeSJVHkknQus8glqXAWuSQVziKXpMJZ5JJUuFonloiI24HXUZ10+VOZuTsiNgO7gZXA3szc1b+YkqTZzDsij4irgd8FLgN+E/jDiNhIdZ7ObcAGYFNEbOlnUEnSzOYt8sz8JvA7mXkMeB7VKP584HBmTrSX7wG29zWpJGlGtaZWMvNoRLwfuBn4AnAhMNmxyiSwtpsNj4+Pd7P6KY1Go9Z6zWazp8dfLMOeby4lZwfzD1LJ2WF483dz8uVbI+JDwN8B66nmy6ctA050s+GxsTFGR0e7uUtX6hb+IDSbzaHON5eSs4P5B6nk7DD4/K1Wa9YBcJ058hdExOUAmfkE8LfANcCajtVWA0fOOqkkqWt1RuSXAu+PiCupRuHbgE8AfxYR64AJYAfVzk9J0iKrs7Pzq8A9wPeAJvDtzPwcsBPYDxwEDgH7+hdTkjSbujs7bwNuO23ZAWDjwkeSJHXDb3ZKUuEsckkqnEUuSYWzyCWpcEu2yKc8f6ekc8SSLfIRz98p6RyxZItcks4VFrkkFc4il6TCWeSSVDiLXJIKZ5FLUuEsckkqnEUuSYWzyCWpcBa5JBWu1oklIuJW4PXtq/dk5rsjYjOwG1gJ7M3MXX3KKEmaQ52TL28GXgm8CLgcaETEG6jO0bkN2ABsiogt/QwqSZpZnamVSeCmzJzKzKPAj4D1wOHMnMjMY8AeYHsfc0qSZjHv1EpmPjR9OSKeTzXF8lGqgp82CaztZsPj4+PdrH5Ko9Ho+j7NZrOnbfXTMGaqq+TsYP5BKjk7DG/+WnPkABHxQuAe4F3AMapR+bRlwIluNjw2Nsbo6Gg3d+lZL+XfT81mc+gy1VVydjD/IJWcHQafv9VqzToArvWplYh4GXAAeE9mfhp4DFjTscpq4MhZ5pQk9WDeEXlEXAR8Cbg2M7/eXvxAdVOsAyaAHVQ7PyVJi6zO1MrNwNOB3RExvezjwE5gf/u2rwL7+pBPkjSPOjs7bwRunOXmjQsbR5LULb/ZKUmFs8glqXAWuSQVziKXpMJZ5JJUOItckgpnkUtS4SxySSqcRS5JhVvyRT519PhTfkrSUrPki3xkxXK23vRlRlYsH3QUSeqLJV/kkrTUnTNF3jnF4nSLpKXknCnyzikWp1skLSXnTJHPxFG6pKXgnC5yR+mSloJaJ1+OiGcD3wZenZmPRMRmYDewEtibmbv6mFGSNId5R+QR8RLgfmB9+/pKqvNzbgM2AJsiYks/Qy6m6amVllMtkgpRZ2rlrcA7gCPt61cAhzNzIjOPAXuA7X3Kt+imp1hGnWqRVIg65+y8HqDjxMsXApMdq0wCa7vd8Pj4eLd3AaDRaPR0v7PRbDaLeMzFUnJ2MP8glZwdhjd/rTny05wHnOy4vgw40e2DjI2NMTo62sPmF99Cv3k0m82BvCEthJKzg/kHqeTsMPj8rVZr1gFwL59aeQxY03F9Nf8/7SJJWmS9jMgfACIi1gETwA6qnZ9L1tTR44ysWH7qpyQNk65H5Jn5JLAT2A8cBA4B+xY21nDxM+aShlntEXlm/lrH5QPAxn4EGmaOzCUNo3P6m53dcmQuaRhZ5JJUOIu8R50H2pKkQerlUysCp1ckDQ1H5GfJY7NIGjSL/Cx5bBZJg2aRS1LhLPI+OH26pXXamYgajcYZy5ySkdQri7wPTp9uGZ3hTESnL3NKRlKvLPIhMtOO0/lG96cvk3TusciHyGyj9W5G95LOPRa5JBXOIl9iup2KOX1nq9M0Unks8iWm1x2tTtNI5bLINauZRveNRqOn0X0vfyl0u75/RehcZZFrVgs5uu/lsfwIp1TPWR00KyJ2ALuAFcBfZuZfLUgqSVJtPY/II+JXgQ8AVwKXA2+LiN9YqGBSL+abppnpW7UzrT/oKaLZbuvMv9jbPtvHmm1arpTnMd9rp+7z6McU4NlMrWwGvp6Z/5WZv6A6b+fratxvOcDU1BStVqunf+evWj7rss7bTl821229rD+sj3UuP4+TJ47xpj+9G9o/T544dsayuW6bXjbXbd0+1rm6bZ/HzMtOnjjWU+9NTU09pUM7LTt58mRPLR4R7wVWZeau9vXrgSsy821z3a/ZbF4JfKunjUqSrmo0Gvd3LjibOfLzgM53gWXAiRr3+y5wFTAJ+DEDSapnObCGqkOf4myK/DGqQp62Gjgy350ajUYLuH++9SRJZ/j3mRaeTZF/DbgtIp4L/AL4PWDOaRVJ0sLreWdnZv4n8D7gG8CDwN9k5r8uVDBJUj097+yUJA0Hv9kpSYWzyCWpcBa5JBXOIpekwp3VQbMGobQDdUXErcDr21fvycx3R8RmYDewEtg7/e3YYRURHwaek5k7S8oeEVuBW4FVwH2ZeWNh+d8IvLd99d7MvHnY80fEs4FvA6/OzEdmyxsRlwOfBJ4N/BNwQ2YeG1DsU2bI/zbgj6i+/PhvwNszc2rY8hc1Ii/tQF3tF/ErgRdR5W1ExBuAO4FtwAZgU0RsGVzKuUXEy4E3ty+vpJDsEXEp8HHgNcBlwIvbWUvJ/wzgI8DVwEbgqvYb09Dmj4iXUH3Zb337+lyvlz3AOzNzPdW3wt+6+Imfaob864F3AS+leg2dB7yjvfpQ5S+qyOn9QF2DMgnclJlTmXkU+BHVi+RwZk6038H3ANsHGXI2EXEB1RvnB9uLrqCQ7MBrqUaAj7V/99cCT1BO/uVU/z9XUf31uQL4OcOd/61URTf9De8ZXy8RcQmwMjP/pb3eXQzH8zg9fwv4g8z8eWaeBH4IXDyM+UubWrmQqhynTVK9WIZSZj40fTkink81xfJRznwOaxc5Wl2foPrS10Xt6zP9/oc1+zpgKiK+AlwM3A08RCH5M/PxiLgFOET1BvRNhvz3n5nXA0TE9KLZ8g7l8zg9f2Y+CjzaXvZc4J3AToYwf2kj8l4P1DVQEfFC4B+o/kz7Dwp4Du2jWf44Mw90LC7p9/80qr/gfh/4LeAlwKUUkj8iLgPeAlxCVRzHqf6aKyJ/22yvl5JeR9NTugeAT2XmPzKE+Usbkfd0oK5BioiXAfuBP87Mz0XE1VRHMJs2rM/hWmBNRDwIXAA8k6pUOo9YOazZAX4CfC0zfwYQEV+k+vO3lPyvAg5k5k8BIuIu4GbKyQ/V/9eZXuuzLR86EfEC4O+Bj2Tmn7cXD13+0oq8qAN1RcRFwJeAazPz6+3FD1Q3xTpgAthBtUNoqGTmK6YvR8RO4BrgBuDwsGdvuxv4dEScDzwObKHap/KeQvJ/H7gjIlZRTa1spXrtXFdIfpjltZ6Zj0bEkxHxssz8Z+BNwL2DDDqTiHgWcB/wvsz8zPTyYcxf1NRKgQfquhl4OrA7Ih5sj253tv/tBw5SzYHuG1TAbmTmkxSSPTMfAO6g+hTCQaq5zo9RTv77gM8CTeAHVDs7b6OQ/DDv6+U64C8i4hDVX3sfGUTGeVwP/Apw0/T/34i4vX3bUOX3oFmSVLiiRuSSpDNZ5JJUOItckgpnkUtS4SxySSqcRS5JhbPIJalwFrkkFe7/APjb/5+zEaXSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('컨테이너', 83),\n",
      " ('도커', 41),\n",
      " ('명령', 34),\n",
      " ('이미지', 34),\n",
      " ('사용', 26),\n",
      " ('가동', 14),\n",
      " ('중지', 13),\n",
      " ('mingw64', 13),\n",
      " ('다음', 12),\n",
      " ('삭제', 12)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(feature_name, count))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'the', 'first', 'document.'],\n",
       " ['This', 'is', 'the', 'second', 'second', 'document.'],\n",
       " ['And', 'the', 'third', 'one.'],\n",
       " ['Is', 'this', 'the', 'first', 'document?'],\n",
       " ['The', 'last', 'document?']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [[text for text in doc.split()] for doc in corpus]\n",
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 0,\n",
       " 'document.': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'the': 4,\n",
       " 'second': 5,\n",
       " 'And': 6,\n",
       " 'one.': 7,\n",
       " 'third': 8,\n",
       " 'Is': 9,\n",
       " 'document?': 10,\n",
       " 'this': 11,\n",
       " 'The': 12,\n",
       " 'last': 13}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(token_list)\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (1, 1), (3, 1), (4, 1), (5, 2)],\n",
       " [(4, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(2, 1), (4, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(10, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix = [dictionary.doc2bow(token) for token in token_list]\n",
    "term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:\n",
      "0 0.49633406058198626\n",
      "1 0.49633406058198626\n",
      "2 0.49633406058198626\n",
      "3 0.49633406058198626\n",
      "4 0.12087183801361165\n",
      "doc:\n",
      "0 0.25482305694621393\n",
      "1 0.25482305694621393\n",
      "3 0.25482305694621393\n",
      "4 0.0620568558708622\n",
      "5 0.8951785160431313\n",
      "doc:\n",
      "4 0.07979258234193365\n",
      "6 0.5755093812740171\n",
      "7 0.5755093812740171\n",
      "8 0.5755093812740171\n",
      "doc:\n",
      "2 0.3485847413542797\n",
      "4 0.08489056411237639\n",
      "9 0.6122789185961829\n",
      "10 0.3485847413542797\n",
      "11 0.6122789185961829\n",
      "doc:\n",
      "10 0.37344696513776354\n",
      "12 0.6559486886294514\n",
      "13 0.6559486886294514\n"
     ]
    }
   ],
   "source": [
    "# TF-ITF 인코딩\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(term_matrix)\n",
    "for doc in tfidf[term_matrix]:\n",
    "    print(\"doc:\")\n",
    "    for k, v in doc:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step1 : 텍스트 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "categories=[\"comp.graphics\", \"rec.sport.baseball\", \"sci.med\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tagged_list = [pos_tag(word_tokenize(doc)) for doc in newsgroups.data]\n",
    "nouns_list = [[t[0] for t in doc if t[1].startswith(\"N\")] for doc in tagged_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "nouns_list = [[lm.lemmatize(w,pos=\"n\") for w in doc] for doc in nouns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_list = [[text.lower() for text in doc] for doc in nouns_list]\n",
    "token_list = [[re.sub(r\"[^A-Za-z]+\", '', word) for word in doc] for doc in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += [\"\", \"subject\", \"article\", \"line\", \"year\", \"month\", \"address\", \"keyword\", \"msg\"]\n",
    "\n",
    "token_list = [[word for word in doc if (word not in stop_words) and (2 < len(word)<10)] for doc in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(token_list)\n",
    "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "model = LdaModel(corpora=doc_term_matrix, id2word=dictionary, num_to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
